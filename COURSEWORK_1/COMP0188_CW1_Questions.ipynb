{"cells":[{"cell_type":"markdown","metadata":{"id":"OJ8J_m5g3yEH"},"source":["# Coursework 1: Chest X-ray (100 marks)\n","\n","In this coursework, you will be working with the Kaggle [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data) dataset. You will analyze the dataset, and train deep learning models to classify whether an x-ray exhibits pneumonia.\n"]},{"cell_type":"markdown","metadata":{"id":"g3nJRr1B3yEK"},"source":["The coursework is structured as follows:\n","\n","1. Data Analysis: 5 marks\n","2. Data Preparation: 5 marks\n","3. Training a Baseline: 30 marks\n","4. Improving the Baseline: 50 marks\n","5. Evaluating on the Test Set: 10 marks\n","\n","In each question will require you tocode up a solution, and to briefly explain and discuss your choices and results.\n","\n","__IMPORTANT__\n","* Pretrained models are __NOT__ allowed. You will recieve __0__ marks for any use of pretrained models.\n","* The use of LLM/AI support including writing and coding aligns to the UCL guidelines. This includes the use of code prompts and Gemini in Google Collab"]},{"cell_type":"markdown","metadata":{"id":"cdf12jco3yEL"},"source":["Here are some additional tips:\n","- We recommend using weights and biases to log your training runs. This will allow you to easily compare previous runs if needed.\n","- Ensure your results are reproducable - we may rerun your notebook to check for this. Points will be lost if results are not reproducable.\n","- We recommend factorizing your code in places where you will be repeatedly using the same functionaility. For example, if you are training multiple models, consider using a common training loop function.\n","- Your code and results and discussions should be concise, well-presented, and easy to read. Each question has a certain portion of marks going towards this.\n","- Ensure you correctly use the train, validation, and test set throughout. You should only ever use the test set once - for the final evaluation.\n","- Consider saving your models so you can reload previous models for the final evaluation\n","- Ensure it is clear to the reader what any plots / figures are presenting. I.e., label axes, include titles, ensure it is clear what experiment it is from (what model / design choices, etc.)\n","- Google Collab restricts the amount of GPU time available. Consider debugging code, using a subset of data, on CPU compute"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aBDQxq2e3yEL"},"outputs":[],"source":["# !pip install kaggle\n","# !pip install wandb"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import kaggle"]},{"cell_type":"markdown","metadata":{"id":"MNGPJcxblyhM"},"source":["If you get the following error when running the import cell below this description:\n","\n","\n","```\n","OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n","```\n","You will need to create a kaggle account, and navigate to https://www.kaggle.com/me/account. Navigate to \"API\" and create a new token. This will automatically download a json file called \"kaggle.json\".\n","\n","Run the following code, replacing the \"INSERT JSON HERE TEXT\" with the contents of the json that you downloaded.\n","\n","```\n","!mkdir /root/.config/kaggle\n","!touch /root/.config/kaggle/kaggle.json\n","\n","api_token = INSERT JSON HERE TEXT\n","\n","import json\n","\n","with open('/root/.config/kaggle/kaggle.json', 'w') as file:\n","    json.dump(api_token, file)\n","\n","!chmod 600 /root/.config/kaggle/kaggle.json\n","```\n","\n","INSERT JSON HERE TEXT should be something of the form:\n","```\n","{\"username\":\"XXX\",\"key\":\"XXX\"}\n","```"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fzTtYHbX3yEM"},"outputs":[],"source":["import os\n","import random\n","import shutil\n","import kaggle\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from collections import Counter\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from torchvision import models, transforms\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","# import wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_2Y454o63yEN"},"outputs":[],"source":["# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"TqB_JTQi3yEN"},"source":["# Load and Re-split the Raw Data\n","\n","The original data is poorly split, so we will resplit it here. Do NOT edit this code."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8e2WyfzM3yEN"},"outputs":[],"source":["# Hyperparameters\n","TRAIN_SPLIT = 0.8\n","VAL_SPLIT = 0.1\n","TEST_SPLIT = 0.1  # This is implicitly defined as 1 - (TRAIN_SPLIT + VAL_SPLIT)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"LEdJHUHQ3yEN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset already exists at chest_xray_dataset. Skipping download.\n"]}],"source":["# Set up the Kaggle API\n","api = KaggleApi()\n","api.authenticate()\n","\n","# Specify the dataset\n","images = \"paultimothymooney/chest-xray-pneumonia\"\n","\n","# Specify the download path\n","download_path = \"chest_xray_dataset\"\n","\n","# Check if the dataset is already downloaded\n","if os.path.exists(os.path.join(download_path, \"chest_xray\")):\n","    print(f\"Dataset already exists at {download_path}. Skipping download.\")\n","else:\n","    # Create the download directory if it doesn't exist\n","    os.makedirs(download_path, exist_ok=True)\n","\n","    # Download the dataset\n","    print(f\"Downloading {images} to {download_path}\")\n","    api.dataset_download_files(images, path=download_path, unzip=True)\n","    print(\"Download complete!\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"knFl_TAC3yEO"},"outputs":[{"name":"stdout","output_type":"stream","text":["Re-split dataset already exists at chest_xray_dataset_new_split\n"]}],"source":["# Re-split data\n","dataset_path = os.path.join(download_path, \"chest_xray\")\n","new_dataset_path = \"chest_xray_dataset_new_split\"\n","\n","if not os.path.exists(new_dataset_path):\n","    # Create new directory structure\n","    for split in ['train', 'val', 'test']:\n","        for cls in ['NORMAL', 'PNEUMONIA']:\n","            os.makedirs(os.path.join(new_dataset_path, split, cls), exist_ok=True)\n","\n","    for cls in ['NORMAL', 'PNEUMONIA']:\n","        all_files = []\n","        for split in ['train', 'val', 'test']:\n","            source_folder = os.path.join(dataset_path, split, cls)\n","            files = os.listdir(source_folder)\n","            all_files.extend([(file, source_folder) for file in files])\n","\n","        # Sort files to ensure consistent order before shuffling\n","        all_files.sort()\n","\n","        # Create a new Random object with the seed\n","        rng = random.Random(42)\n","\n","        # Use the shuffle method of the Random object\n","        rng.shuffle(all_files)\n","\n","        total_files = len(all_files)\n","        train_end = int(total_files * TRAIN_SPLIT)\n","        val_end = int(total_files * (TRAIN_SPLIT + VAL_SPLIT))\n","\n","        train_files = all_files[:train_end]\n","        val_files = all_files[train_end:val_end]\n","        test_files = all_files[val_end:]\n","\n","        for split, file_list in [('train', train_files), ('val', val_files), ('test', test_files)]:\n","            for file, source_folder in file_list:\n","                source = os.path.join(source_folder, file)\n","                dest = os.path.join(new_dataset_path, split, cls, file)\n","                shutil.copy(source, dest)\n","\n","    print(f\"Data re-split complete. New dataset location: {new_dataset_path}\")\n","else:\n","    print(f\"Re-split dataset already exists at {new_dataset_path}\")"]},{"cell_type":"markdown","metadata":{"id":"BCEwmeP03yEO"},"source":["# Question 1: Data Analysis (5 marks)\n","\n","Perform some basic analysis of the statistics of the dataset.\n","\n","Try to spot anything that may impact how you will design your deep learning classifier and training.\n","\n","We'd expect to see:\n","* Analysis of labels (target variable);\n","* Analysis of input features;\n","\n","If you do spot anything, briefly explain **how you think it may impact training**.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"fcFr_0-h3yEP"},"outputs":[],"source":["# Collect dataset statistics\n","splits = ['train', 'val', 'test']\n","classes = ['NORMAL', 'PNEUMONIA']\n","\n","stats = {split: {cls: 0 for cls in ['NORMAL', 'PNEUMONIA']} for split in ['train', 'val', 'test']}\n","for split in splits:\n","    for cls in classes:\n","        path = os.path.join(new_dataset_path, split, cls)\n","        stats[split][cls] = len(os.listdir(path))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Image(os.path.join(new_dataset_path, ''))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bOewT8sA3yEP"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'train': {'NORMAL': 1266, 'PNEUMONIA': 3418}, 'val': {'NORMAL': 158, 'PNEUMONIA': 427}, 'test': {'NORMAL': 159, 'PNEUMONIA': 428}}\n","Original DataFrame:\n","   Split      Class  Count\n","0  train     NORMAL   1266\n","1  train  PNEUMONIA   3418\n","2    val     NORMAL    158\n","3    val  PNEUMONIA    427\n","4   test     NORMAL    159\n","5   test  PNEUMONIA    428\n","\n","Pivoted DataFrame:\n","   Split  NORMAL  PNEUMONIA\n","0   test     159        428\n","1  train    1266       3418\n","2    val     158        427\n"]},{"data":{"text/plain":["array([[<Axes: title={'center': 'NORMAL'}>,\n","        <Axes: title={'center': 'PNEUMONIA'}>]], dtype=object)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","\n","print(stats)\n","\n","images = []\n","for split in stats:\n","    for cls in stats[split]:\n","        images.append({\n","            'Split': split,\n","            'Class': cls,\n","            'Count': stats[split][cls]\n","        })\n","\n","# Create the DataFrame\n","df = pd.DataFrame(images)\n","\n","# Optionally, you can pivot the DataFrame for a different view\n","df_pivot = df.pivot(index='Split', columns='Class', values='Count').reset_index()\n","df_pivot.columns.name = None  # Remove the columns name\n","\n","# Print both DataFrames\n","print(\"Original DataFrame:\")\n","print(df)\n","print(\"\\nPivoted DataFrame:\")\n","print(df_pivot)\n","df_pivot.hist()"]},{"cell_type":"markdown","metadata":{"id":"HiuMf1ce3yEP"},"source":["**(a)**\n","\n","_Insert brief discussion of analysis here_"]},{"cell_type":"markdown","metadata":{"id":"SDN6YQeP3yEP"},"source":["# Question 2: Data Preparation (5 marks)\n","\n","Here, you should load the dataset into torch dataloaders, performing any preprocessing required in the process.\n","\n","Within the ChestXrayDataset class, the root_dir parameter is a string defining the directory containing the \"train\", \"val\" and \"test\" folders."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"NcBeWbFP3yEP"},"outputs":[],"source":["dataset_path = \"chest_xray_dataset_new_split\"\n","BATCH_SIZE: int = 32"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"atk7VPI23yEQ"},"outputs":[],"source":["from typing import Dict, List, Optional, Tuple, Union\n","\n","import torchvision\n","\n","class ChestXrayDataset(Dataset):\n","    \"\"\"Chest Xray Dataset Loader\"\"\"\n","    def __init__(self, root_dir: str, split: str, transform: torchvision.transforms.Compose = None):\n","        \"\"\"Provide the root dir to the dataset and the specified split\"\"\"\n","        super().__init__()        \n","        self.root_dir: str = root_dir\n","        self.classes = ['NORMAL', 'PNEUMONIA']\n","        self.split = split\n","        self.transform = transform\n","        #Fuck this pathing nonsense. Lets just put everything in a dict.\n","        self.images: dict = self.load_image_paths()\n","        \n","    def load_image_paths(self) -> Dict[str, List[str]]:\n","        \"\"\"\n","        Load image paths from the dataset directory structure.\n","        \n","        Returns:\n","        Dict[str, List[str]]: A dictionary containing image paths for 'NORMAL', 'PNEUMONIA', and 'ALL' categories.\n","        \"\"\"\n","        \n","        images = {'NORMAL': [], 'PNEUMONIA': [], 'ALL': []}\n","        \n","        for cls in self.classes:\n","            class_dir = os.path.join(self.root_dir, self.split, cls)\n","            image_paths = [\n","                os.path.join(class_dir, img)\n","                for img in os.listdir(class_dir)\n","                if img.lower().endswith(('.png', '.jpg', '.jpeg'))\n","            ]\n","            images[cls] = image_paths\n","            images['ALL'].extend(image_paths)  # Add paths to 'ALL' category\n","        \n","        return images\n","            \n","    def summary(self) -> None:\n","        print(f\"\"\"\n","---- *{self.split}* Dataset Summary ----\n","Found {len(self.images['NORMAL'])} NORMAL images\n","Found {len(self.images['PNEUMONIA'])} PNEUMONIA images\n","Found {len(self.images['ALL'])} total images\n","              \"\"\")\n","    def __len__(self) -> int:\n","        # Sum of both image sets\n","        return len(self.images['ALL']) \n","\n","    def __getitem__(self, idx) -> Tuple[str | None, Image.Image | torch.Tensor]:\n","        \"\"\"Returns label, and either a PIL image or a tensor transformations applied\"\"\"\n","    \n","        image_path: str = self.images['ALL'][idx]\n","        # Open image in grayscale mode with PIL\n","        image = Image.open(image_path).convert('L')\n","        \n","        label = 0\n","        if 'NORMAL' in image_path:\n","            label = 0\n","        elif 'PNEUMONIA' in image_path:\n","            label = 1\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","        else:\n","            # If no transform is provided, convert to tensor manually\n","            image = torch.from_numpy(np.array(image)).float().unsqueeze(0) / 255.0\n","        \n","        return label, image"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","---- *train* Dataset Summary ----\n","Found 1266 NORMAL images\n","Found 3418 PNEUMONIA images\n","Found 4684 total images\n","              \n","0 torch.Size([1, 1317, 1857])\n"]}],"source":["xray_example_dataset = ChestXrayDataset(dataset_path, split='train')\n","xray_example_dataset.summary()\n","label, tensor = xray_example_dataset[0]\n","print(label, tensor.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"DMEI4zhT3yEQ"},"outputs":[],"source":["# Define data transforms\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","\n","# From Lab 3\n","def compute_mean_std(dataset: ChestXrayDataset):\n","    total_sum = 0\n","    total_squared_sum = 0\n","    total_pixels = 0\n","    \n","    for image_idx in range(0, len(dataset)):\n","        image_tensor = dataset[image_idx]\n","        total_sum += torch.mean(image_tensor)\n","        total_squared_sum += torch.mean(image_tensor**2)\n","        total_pixels += image_tensor.numel()\n","    \n","    mean = total_sum / len(dataset)\n","    std = (total_squared_sum / len(dataset) - mean ** 2) ** 0.5\n","    \n","    print(mean.item(), std.item())\n","    return mean.item(), std.item()\n","\n","# mean, std = compute_mean_std(xray_example_dataset)\n","\n","\"\"\"Transform compose that will be calculated using train set.\"\"\"\n","xray_train_data_transforms = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    # transforms.Normalize((mean,), (std,))\n","])\n","# TODO: Move the normalization to the 4th step.\n","\n","\n","    ########################################################################\n","    #                             END OF YOUR CODE                         #\n","    ########################################################################"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"0fa080DI3yEQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","---- *train* Dataset Summary ----\n","Found 1266 NORMAL images\n","Found 3418 PNEUMONIA images\n","Found 4684 total images\n","              \n","\n","---- *test* Dataset Summary ----\n","Found 159 NORMAL images\n","Found 428 PNEUMONIA images\n","Found 587 total images\n","              \n","\n","---- *val* Dataset Summary ----\n","Found 158 NORMAL images\n","Found 427 PNEUMONIA images\n","Found 585 total images\n","              \n"]},{"data":{"text/plain":["'???'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","from torch.utils.data import DataLoader\n","\n","\"\"\"I'm using the mean and std from the training dataset to avoid biases.\n","Like we talked about in class.\"\"\"\n","# Create datasets\n","xray_train_dataset = ChestXrayDataset(dataset_path, split='train', transform=xray_train_data_transforms)\n","xray_test_dataset = ChestXrayDataset(dataset_path, split='test', transform=xray_train_data_transforms)\n","xray_val_dataset = ChestXrayDataset(dataset_path, split='val', transform=xray_train_data_transforms)\n","\n","\n","# Create data loaders\n","xray_train_loader = DataLoader(dataset=xray_train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n","xray_test_loader = DataLoader(dataset=xray_test_dataset, batch_size=BATCH_SIZE, shuffle=True) \n","xray_val_loader = DataLoader(dataset=xray_val_dataset, batch_size=BATCH_SIZE, shuffle=True) \n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################\n","\n","# Print dataset sizes\n","xray_train_dataset.summary()\n","xray_test_dataset.summary()\n","xray_val_dataset.summary()\n","\n","# Print batch shapes\n","\"\"\"???\"\"\""]},{"cell_type":"markdown","metadata":{"id":"ZelQODT83yEQ"},"source":["**(a)**\n","\n","_Insert brief discussion of any design choices you made here_\n","\n","__Response:__ I chose to calculate the mean and std using the train dataset. I also realized our xray images are all grayscale and have decided to load all of them as such. For example, in RGB, white is (255,255,255) black is (0,0,0) grey is (100,100,100) etc... There's no point having three channels if they are all going to be the same value. I figure this will save on computation and complexity.\n"]},{"cell_type":"markdown","metadata":{"id":"ty37YSYK3yEQ"},"source":["# Question 3: Training a Baseline (30 marks)\n","\n","You will now establish an initial baseline model and training procedure. This should be as simple as possible, without using any elaborate design choices, whilst still obtaining reasonable performance (i.e., at least better than random chance). You will attempt to improve upon this baseline in later questions.\n","\n","When answering this question, consider what makes a good baseline:\n","* Easily converges;\n","* Easy to implement;\n","* Established architectural components that have proved well suited to the data-type and problem.\n","* Obtains reasonable performance e.g, better than random guess\n","\n","You will be required to explain your design choices, and to present and discuss you results.\n","\n","The code below is a suggested structure to guide you. You are free to deviate from this __however, it must be obvious to the marker__:\n","* What the final proposed baseline model is (in terms of architecture);\n","* What the performance of the baseline model is and how the model has been trained;\n","* What your written justification and discussion is;\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"RzUuFDRc3yEQ"},"outputs":[],"source":["NUM_EPOCHS = 1\n","use_wandb = False  # Set to True if you want to use wandb\n","lr = 1e8"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2j6klXIC3yEQ"},"outputs":[],"source":["# Define the model\n","class SimpleModel(nn.Module):\n","\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.model = nn.Sequential(\n","            # First Convolutional Layer\n","            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            \n","            # Second Convolutional Layer\n","            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            \n","            # Flatten layer\n","            nn.Flatten(),\n","            \n","            # Fully Connected Layers\n","            nn.Linear(32 * 56 * 56, 128),  # Assuming input image size is 224x224\n","            nn.ReLU(),\n","            nn.Linear(128, 2)  # 2 output classes: Normal and Pneumonia\n","        )\n","        \n","    def forward(self, x):\n","        return self.model(x)\n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################\n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"Y0xh1jau3yER"},"outputs":[],"source":["def calculate_class_accuracy(outputs: torch.tensor, labels: torch.tensor):\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","        \n","    # Convert outputs to predicted class (0 or 1)\n","    # _, predicted = torch.max(outputs, 1)\n","    \n","    # Calculate overall accuracy\n","    correct = (outputs == labels).sum().item()\n","    total = labels.size(0)\n","    overall_accuracy = correct / total\n","    \n","    # Calculate accuracy for each class\n","    class_correct = [0, 0]  # [NORMAL correct, PNEUMONIA correct]\n","    class_total = [0, 0]    # [NORMAL total, PNEUMONIA total]\n","    \n","    for i in range(total):\n","        label = labels[i]\n","        class_correct[label] += (outputs[i] == label).item()\n","        class_total[label] += 1\n","    \n","    normal_accuracy = class_correct[0] / class_total[0] if class_total[0] > 0 else 0\n","    pneumonia_accuracy = class_correct[1] / class_total[1] if class_total[1] > 0 else 0\n","    \n","    return {\n","        'overall': overall_accuracy,\n","        'NORMAL': normal_accuracy,\n","        'PNEUMONIA': pneumonia_accuracy\n","    }\n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"7EPsfC5s3yER"},"outputs":[],"source":["from torch.optim import Optimizer\n","from torch.nn.modules.loss import _Loss\n","from tqdm import tqdm\n","\n","def train_model(model: nn.Module,\n","                train_loader: DataLoader,\n","                val_loader: DataLoader,\n","                criterion: _Loss,\n","                optimizer: Optimizer,\n","                NUM_EPOCHS=1,\n","                device='cuda',\n","                use_wandb=False) -> Tuple[\n","    List[float],\n","    List[float],\n","    List[float],\n","    List[float],\n","    List[Dict[str, float]],\n","    List[Dict[str, float]]\n","]:\n","\n","    model.train()\n","\n","    \"\"\"Losses and accuracies\"\"\"\n","    train_losses = []\n","    train_accuracies = []\n","    val_losses = []\n","    val_accuracies = []\n","    train_class_accuracies = []\n","    val_class_accuracies = []\n","    \n","    for epoch in range(NUM_EPOCHS):\n","        # Training phase\n","        model.train()\n","        train_loss = 0.0\n","        train_predictions = []\n","        train_targets = []\n","        val_predictions = []\n","        val_targets = []\n","        \n","        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n","        for labels, images in train_pbar:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            # print(labels)\n","            \n","            # Forward pass\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            \n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            \n","            # Track metrics\n","            train_loss += loss.item()\n","            train_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n","            train_targets.extend(labels.cpu().numpy())\n","            # print(train_targets)\n","            # Update progress bar\n","            train_pbar.set_postfix({'loss': loss.item()})\n","            \n","            \n","        epoch_avg_loss = train_loss / len(train_loader)\n","        epoch_accuracy = calculate_class_accuracy(\n","            torch.tensor(train_predictions), \n","            torch.tensor(train_targets)\n","        )\n","        train_accuracies.append(epoch_accuracy['overall'])\n","        train_losses.append(epoch_avg_loss)\n","        \n","        # Validation phase\n","        model.eval()  # Set model to evaluation mode\n","        val_loss = 0.0\n","        val_predictions = []\n","        val_targets = []\n","        \n","        with torch.no_grad():  # No gradient computation needed for validation\n","            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} Validation\")\n","            for labels, images in val_pbar:\n","                # images = images.to(device)\n","                # labels = labels.to(device)\n","                \n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                \n","                val_loss += loss.item()\n","                val_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n","                val_targets.extend(labels.cpu().numpy())\n","                val_pbar.set_postfix({'loss': loss.item()})\n","        \n","        # Calculate validation metrics for epoch\n","        epoch_val_loss = val_loss / len(val_loader)\n","        epoch_val_accuracy = calculate_class_accuracy(\n","            torch.tensor(val_predictions), \n","            torch.tensor(val_targets)\n","        )\n","        val_accuracies.append(epoch_val_accuracy['overall'])\n","        val_class_accuracies.append(epoch_val_accuracy)\n","        val_losses.append(epoch_val_loss)\n","\n","        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n","        print(f\"Train Loss: {epoch_avg_loss:.4f}, Train Acc: {epoch_accuracy['overall']:.4f}\")\n","        print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy['overall']:.4f}\")\n","        \n","\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","\n","# Feel free to add more intermediate functions to do this\n","\n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################\n","\n","    return train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ONedb4HQ3yER"},"outputs":[],"source":["import seaborn as sns\n","def plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies):\n","\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","    \n","    # Set the style\n","    plt.style.use('seaborn')\n","    \n","    # Create a figure with 2x2 subplots\n","    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n","    \n","    # Plot training and validation loss\n","    epochs = range(1, len(train_losses) + 1)\n","    \n","    ax1.plot(epochs, train_losses, 'b-', label='Training Loss')\n","    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss')\n","    ax1.set_title('Loss vs. Epochs')\n","    ax1.set_xlabel('Epochs')\n","    ax1.set_ylabel('Loss')\n","    ax1.legend()\n","    ax1.grid(True)\n","    \n","    # Plot training and validation accuracy\n","    ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n","    ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n","    ax2.set_title('Accuracy vs. Epochs')\n","    ax2.set_xlabel('Epochs')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.legend()\n","    ax2.grid(True)\n","    \n","    # Plot per-class training accuracies\n","    if train_class_accuracies:\n","        for class_name, accuracies in train_class_accuracies.items():\n","            ax3.plot(epochs, accuracies, label=f'Class {class_name}')\n","        ax3.set_title('Training Accuracy per Class')\n","        ax3.set_xlabel('Epochs')\n","        ax3.set_ylabel('Accuracy')\n","        ax3.legend()\n","        ax3.grid(True)\n","    \n","    # Plot per-class validation accuracies\n","    if val_class_accuracies:\n","        for class_name, accuracies in val_class_accuracies.items():\n","            ax4.plot(epochs, accuracies, label=f'Class {class_name}')\n","        ax4.set_title('Validation Accuracy per Class')\n","        ax4.set_xlabel('Epochs')\n","        ax4.set_ylabel('Accuracy')\n","        ax4.legend()\n","        ax4.grid(True)\n","    \n","    # Adjust layout to prevent overlap\n","    plt.tight_layout()\n","    \n","    return fig        \n","        \n","# Add all your plotting code here. Feel free to use intermediate functions\n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"C5bwRdrU3yER"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/1: 100%|| 147/147 [00:38<00:00,  3.83it/s, loss=0.323] \n","Epoch 1/1 Validation:   0%|          | 0/585 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[55], line 26\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m     16\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,              \u001b[38;5;66;03m# Learning rate - start with 0.01\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,         \u001b[38;5;66;03m# Momentum - helps with faster convergence\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m   \u001b[38;5;66;03m# L2 regularization - helps prevent overfitting\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m#                             END OF YOUR CODE                         #\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m########################################################################\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxray_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxray_val_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest_xray_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[1;32mIn[54], line 84\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, use_wandb)\u001b[0m\n\u001b[0;32m     79\u001b[0m val_pbar \u001b[38;5;241m=\u001b[39m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Validation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labels, images \u001b[38;5;129;01min\u001b[39;00m val_pbar:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# images = images.to(device)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# labels = labels.to(device)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     87\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[17], line 30\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"]}],"source":["# Initialize the model\n","model = SimpleModel().to(device)\n","\n","\n","# Define loss function and optimizer\n","        ########################################################################\n","        #                              YOUR CODE HERE                          #\n","        ########################################################################\n","\n","# Binary Cross Entropy Loss is appropriate for binary classification\n","criterion = nn.CrossEntropyLoss()\n","\n","# SGD optimizer with some reasonable initial hyperparameters\n","optimizer = optim.SGD(\n","    model.parameters(),\n","    lr=0.01,              # Learning rate - start with 0.01\n","    momentum=0.9,         # Momentum - helps with faster convergence\n","    weight_decay=0.0001   # L2 regularization - helps prevent overfitting\n",")\n","\n","        ########################################################################\n","        #                             END OF YOUR CODE                         #\n","        ########################################################################\n","\n","#Train the model\n","train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies = train_model(\n","    model, xray_train_loader, xray_val_dataset, criterion, optimizer, NUM_EPOCHS, device, use_wandb\n",")\n","\n","# Plot training curves\n","\n","# Save the model\n","torch.save(model.state_dict(), 'chest_xray_model.pth')\n","\n","# plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies)\n","print(\"Model saved as 'chest_xray_model.pth'\")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["[0.3386991332430823]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies, train_class_accuracies, val_class_accuracies)\n"]},{"cell_type":"markdown","metadata":{"id":"kPUy57xr3yER"},"source":["**(a)**\n","\n","_Insert brief explanation of the design choices you made_\n","\n","__Response:__ I started with a basic CNN that we used for Lab 3. That is, a couple of 2D Kernels, flatenning, and then a couple of linear layers. The results in Lab 3, to me, were very impressive with this basic model. So I'm thinking this will be a good Baseline. Mostly because I know I will be building off a CNN anyways."]},{"cell_type":"markdown","metadata":{"id":"wy-HoUkkecEr"},"source":["**(b)**\n","\n","_Present your results, including plots etc. here_"]},{"cell_type":"markdown","metadata":{"id":"E--x4N1w3yES"},"source":["**(c)**\n","\n","_Discuss your results here_"]},{"cell_type":"markdown","metadata":{"id":"sq5YLFof3yES"},"source":["# Question 4: Improving the Baseline (50 marks)\n","After analysing the results of your baseline, can you spot any clear areas for improvement, or think of any obvious improvements to your model and training setup that will improve performance?\n","\n","You are free to try out as many improvements as you want here. You may also try modifying aspects of the data.\n","\n","**However, for the final code and results you present in your submission, you should use exactly 3 design choices which (attempt to) improve upon the baseline.**"]},{"cell_type":"markdown","metadata":{"id":"s8C8cR5govik"},"source":["Tips:\n","* If you struggle to improve upon the baseline, but your design choices are well motivated and well implemented, and your results are well-presented and discussed, you will still receive most marks here. You will get some extra marks for improving upon baseline performance, but you will primarily be marked for making reasonable design choices.\n","* A small number of marks will be deducted if there are extremely obvious issues with the baseline that you do not attempt to address"]},{"cell_type":"markdown","metadata":{"id":"gJb8yLNffD22"},"source":["## Q 4.1: Final improved model -- baseline + 3 improvements (20 marks)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b0ARZtLmgDGh"},"source":["You should now choose three final improvements. Explain them, implement them, train a model, and present and discuss the results.\n","\n","Try to maximize performance with the final three improvements you choose (i.e., pick the three best improvements you found)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0e1-IBUhg_Xj"},"outputs":[],"source":["# Implement the improvements and train the model in as many cells as you need.\n","\n","########################################################################\n","#                              YOUR CODE HERE                          #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"Wegv4NKFg3W7"},"source":["**(a)**\n","\n","*Insert a brief explanation of the three improvements you have used*\n","\n","*For each improvment:*\n","1. *State the change being made;*\n","2. *State **why** this change could, in theory, improve the performance of the baseline model. If possible, motivate your hypothesis using empirical evidence from the baseline models results*"]},{"cell_type":"markdown","metadata":{"id":"lZc77acWhMEM"},"source":["**(b)**\n","\n","_Present your results, including plots etc, here_\n","\n","(Hint: ensure you compare to the baseline)"]},{"cell_type":"markdown","metadata":{"id":"V7z2sLUxhWhd"},"source":["**(c)**\n","\n","_Discuss your results here_"]},{"cell_type":"markdown","metadata":{"id":"CIj57HG2fWll"},"source":["## Q 4.2: Empirically justify improvement 1 (10 marks)"]},{"cell_type":"markdown","metadata":{"id":"CGgAxZ3hlwVa"},"source":["Now you will empirically demonstrate the contribution of each improvement to the final performance of your model.\n","\n","To justify the utility of an improvement, you should present one of the following experiments:\n","- *Option 1:* Train the final model _without_ that improvement (but still with the other two improvements). Compare these results to the results you presented previously with all three improvements. If the improvement is useful, removing it should result in a drop in performance\n","- *Option 2:* Compare the performance of baseline to the perfroamnce of the baseline plus a single improvement. If the improvement is useful, you should expect improved performance versus the baseline.\n","\n","You will still get a significant portion of the marks if the proposed improvement was well-motivated but does not empirically improve perfromance. In this case, ensure your discussion touches on why performance may not have improved or any other interesting talking points.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jA2yiCs2nnMp"},"outputs":[],"source":["# Implement the experiment in as many cells as you need.\n","\n","########################################################################\n","#                              YOUR CODE HERE                          #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"Xk8Uzkgmn4gZ"},"source":["**(a)**\n","\n","_State the improvement you are justifying_"]},{"cell_type":"markdown","metadata":{"id":"iCYUpvoYnpbg"},"source":["**(b)**\n","\n","_Present your results, including plots etc, here_"]},{"cell_type":"markdown","metadata":{"id":"QEIUdWktnu4Q"},"source":["**(c)**\n","\n","_Discuss your results here_"]},{"cell_type":"markdown","metadata":{"id":"6vydel9XfjUv"},"source":["## Q 4.3: Empirically justify improvement 2 (10 marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_tnhBmLoN2a"},"outputs":[],"source":["# Implement the experiment in as many cells as you need.\n","\n","########################################################################\n","#                              YOUR CODE HERE                          #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"IKvOHbc0oEHU"},"source":["**(a)**\n","\n","_State the improvement you are justifying_"]},{"cell_type":"markdown","metadata":{"id":"qh-bK3ScoEHc"},"source":["**(b)**\n","\n","_Present your results, including plots etc, here_"]},{"cell_type":"markdown","metadata":{"id":"Lp8RIsSsoEHc"},"source":["**(c)**\n","\n","_Discuss your results here_"]},{"cell_type":"markdown","metadata":{"id":"goZoEDK-fk0-"},"source":["## Q 4.4: Empirically justify improvement 3 (10 marks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxyMLFtUoOz6"},"outputs":[],"source":["# Implement the experiment in as many cells as you need.\n","\n","########################################################################\n","#                              YOUR CODE HERE                          #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"Tzu22nM4oCVK"},"source":["**(a)**\n","\n","_State the improvement you are justifying_"]},{"cell_type":"markdown","metadata":{"id":"-Ywu9x_joCVT"},"source":["**(b)**\n","\n","_Present your results, including plots etc, here_"]},{"cell_type":"markdown","metadata":{"id":"ILe3TKZgoCVT"},"source":["**(c)**\n","\n","_Discuss your results here_"]},{"cell_type":"markdown","metadata":{"id":"JrteSKJP3yEW"},"source":["# Question 5: Final Evaluation (10 marks)\n","\n","You should perform a final evaluation of the performance of your model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPAaD8zD3yEW"},"outputs":[],"source":["# Implement evaluation here\n","\n","########################################################################\n","#                              YOUR CODE HERE                          #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"UVzR_Weo3yEW"},"source":["**(a)**\n","\n","_Present your results, including plots etc, here_"]},{"cell_type":"markdown","metadata":{"id":"AVQfQU0puHLJ"},"source":["**(b)**\n","\n","_Discuss your results here_"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
