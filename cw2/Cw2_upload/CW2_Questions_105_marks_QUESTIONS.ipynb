{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Robotic behavioural cloning from images and actuator data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "For this coursework, you are required to develop a behaviour cloning model. Behaviour cloning is a technique for training robotic agents using a dataset of sequences of actions. Consider the supervised learning definition where we have a dataset of observatios $d=\\{(x_{1},y_{1}),...,(x_{n},y_{n})\\}$ and the aim is to learn a function: $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$. In this case, $\\mathcal{X}$ is the set of \"observations\" that the robot makes and $\\mathcal{Y}$ is the set of actions that the robot takes.\n",
    "\n",
    "The dataset you have been provided with contains examples of robot arms being required to pickup objects or drop objects (given it has already picked the object up) in a specific place. The observation space ($\\mathcal{X}$) consists of:\n",
    "- \"front_cam_ob\": A 3rd person image of the scene \n",
    "- \"mount_cam_ob\": An image of the scene taken from a camera mounted on top of the robotic arm, looking down\n",
    "- \"ee_cartesian_pos_ob\": The positional and orientation co-ordinates of the robotic arm\n",
    "- \"ee_cartesian_vel_ob\": The velocity of position and orientation of the robotic arm\n",
    "- \"joint_pos_ob\": The position of the gripper which opens and closes\n",
    "\n",
    "The action space ($\\mathcal{Y}$) consists of:\n",
    "- Three co-ordinates defining how much to move the robotic arm\n",
    "- An action defining whether to open, not to move or close the gripper\n",
    "\n",
    "The dataset is split into \"trajectories\" i.e., sequences of:\n",
    "- $x_{i}$: The front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob, joint_pos_ob __at time point i__ \n",
    "- $y_{i}$: The action taken i.e., how to move the arm and the gripper __given the observations__ in $x_{i}$\n",
    "\n",
    "More information on the dataset can be found at: https://github.com/clvrai/clvr_jaco_play_dataset?tab=readme-ov-file\n",
    "\n",
    "### Task\n",
    "Your task has been split into several questions, each exploring how to develop an appropriate model for learning $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$. The task will require you to:\n",
    "- Question 1: Tune an end-to-end supervised learning model taking in the full set of observations and predicting actions: You will be required to evaluate a proposed deep learning architecture (which takes as input all of front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob and predict the two actions) and propose a new model which outperforms the existing model;\n",
    "- Question 2: Define and evaluate a VAE model for performing self-supervised learning and tune it as best you can, to learn a latent representation that can be used as input to a downstream supervised model for behaviour cloning\n",
    "- Question 3: Evaluate the performance of your model proposed in question 1 against your self-supervised VAE representations from question 2 (plus a supervised head) on the test set\n",
    "\n",
    "### Pointers\n",
    "Some helper functions have been provided for you including the following functionality:\n",
    "- A training and validation loop capabale of:\n",
    "  - Handling \"half-precision\" modelling;\n",
    "  - Logging results to weights and biases;\n",
    "- An eda template to help you visualise the data\n",
    "- An evaluation template to help you load saved model checkpoints from weights and biases\n",
    "- A preprocessing script to help you convert the data into train/validation and test splits;\n",
    "  - In this preprocessing script, trajectories longer than 75 timesteps have been removed to ease the computational requirements of the task;\n",
    "- A torch Dataset class capable of handling the multi-model nature of the data;\n",
    "- A example collate_fn to use in Dataloaders\n",
    "\n",
    "Additionally, it is strongly suggested to call ```torch.manual_seed(1)``` whenever you initialise your model (i.e., when you first create the model or call model.reset()). This will ensure the parameters are initialised at the same value each time.\n",
    "\n",
    "### IMPORTANT\n",
    "- You are __not__ allowed to use pre-trained models, developed outside of this coursework i.e., you could __not__ use a pre-trained YOLO model\n",
    "- Questions have been marked under the title \"Task\", ensure that you answer/address all of the bullet points under these headings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "Download the data for the project from here: https://drive.usercontent.google.com/download?id=1tVSilmXhyQd8gxZAEhvKMnynw0qzRFSZ&authuser=0\n",
    "\n",
    "Save the data locally at: ../data/all_play_data_diverse or in Google Collab at: /content/drive/MyDrive/comp0188_2425/cw2. Saving the data in these locations will ensure the proprocessing script provided runs correctly. If you would like to alter these locations, you can alter them in the config.py file of the provided comp0188_cw2 package via the ROOT_PATH global variable.\n",
    "\n",
    "### transition_df.csv\n",
    "You have additionally been provided with a csv called \"transition_df.csv\". This contains a row for each observation/action pair in the dataset and is used to generate the train/validation and test datasets for this task. Note that this csv contains all trajectories (even those over 75 timesteps). This csv might also be useful for EDA. The transition_df.csv should be placed in the same folder that you placed the raw data in (discussed above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comp0188_cw2 contains some config functionality so that you can run the code in collab and locally (to test models)! When you first import the package, import the project_options and set the appropriate configs.\n",
    "- project_options.collab = True will set the dataset directories for google collab whilst false will set suitable local directories\n",
    "- project_options.debug = True will load a subset of data whilst False will load all of the data. \n",
    "\n",
    "__IMPORTANT__: Alterting these options __once you have loaded other functionality__ from comp0188_cw2 may result in unintended outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/joshuaspear/pymlrf.git\n",
      "  Cloning https://github.com/joshuaspear/pymlrf.git to c:\\users\\gugu\\appdata\\local\\temp\\pip-req-build-qmyf52at\n",
      "  Resolved https://github.com/joshuaspear/pymlrf.git to commit 2e86313edd7900c1b5621b4e31368a36328b69f8\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pymlrf\n",
      "  Building wheel for pymlrf (pyproject.toml): started\n",
      "  Building wheel for pymlrf (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pymlrf: filename=pymlrf-1.0.0-py3-none-any.whl size=20654 sha256=5354c57e44c68df509a5ad534297ea313def2f94a60b42b6e0a062b239e871df\n",
      "  Stored in directory: C:\\Users\\Gugu\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-iuf5lbj9\\wheels\\26\\53\\d8\\3a38bb165462bfc461c71eea0e1a42850e7ac695b9a21ebdd5\n",
      "Successfully built pymlrf\n",
      "Installing collected packages: pymlrf\n",
      "  Attempting uninstall: pymlrf\n",
      "    Found existing installation: pymlrf 1.0.0\n",
      "    Uninstalling pymlrf-1.0.0:\n",
      "      Successfully uninstalled pymlrf-1.0.0\n",
      "Successfully installed pymlrf-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/joshuaspear/pymlrf.git 'C:\\Users\\Gugu\\AppData\\Local\\Temp\\pip-req-build-qmyf52at'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "                                              0.0/3.0 MB ? eta -:--:--\n",
      "     --                                       0.2/3.0 MB 5.9 MB/s eta 0:00:01\n",
      "     --------                                 0.7/3.0 MB 8.4 MB/s eta 0:00:01\n",
      "     --------------                           1.1/3.0 MB 8.9 MB/s eta 0:00:01\n",
      "     ----------------------                   1.7/3.0 MB 9.9 MB/s eta 0:00:01\n",
      "     --------------------------------         2.5/3.0 MB 11.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   2.9/3.0 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.0/3.0 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\gugu\\desktop\\ucl\\deep_learning\\.venv\\lib\\site-packages (from h5py) (1.26.3)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\gugu\\desktop\\ucl\\deep_learning\\.venv\\lib\\site-packages (1.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaxtyping in c:\\users\\gugu\\desktop\\ucl\\deep_learning\\.venv\\lib\\site-packages (0.2.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/joshuaspear/comp0188_cw2_public.git\n",
      "  Cloning https://github.com/joshuaspear/comp0188_cw2_public.git to c:\\users\\gugu\\appdata\\local\\temp\\pip-req-build-virgto_8\n",
      "  Resolved https://github.com/joshuaspear/comp0188_cw2_public.git to commit 5057522998ce2732f354cd1bc232908c823f6960\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/joshuaspear/comp0188_cw2_public.git 'C:\\Users\\Gugu\\AppData\\Local\\Temp\\pip-req-build-virgto_8'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typeguard==2.13.3 in c:\\users\\gugu\\desktop\\ucl\\deep_learning\\.venv\\lib\\site-packages (2.13.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "COLLAB = False\n",
    "# if COLLAB:\n",
    "!pip install --force-reinstall git+https://github.com/joshuaspear/pymlrf.git\n",
    "!pip install h5py\n",
    "# !pip install wandb\n",
    "!pip install torchinfo\n",
    "!pip install jaxtyping\n",
    "!pip install git+https://github.com/joshuaspear/comp0188_cw2_public.git\n",
    "!pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the error: \n",
    "```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from comp0188_cw2 import project_options\n",
    "project_options.collab = False\n",
    "print(project_options.collab)\n",
    "project_options.debug = True\n",
    "print(project_options.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "You need to create the file path: ../data/all_play_data_diverse",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymlrf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStructs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetOutput\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomp0188_cw2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_all_files\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\comp0188_cw2\\utils.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WANDB_PROJECT\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@jaxtyped\u001b[39m(typechecker\u001b[38;5;241m=\u001b[39mtypechecker)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_greyscale\u001b[39m(\n\u001b[0;32m     18\u001b[0m     c_img:Int[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size row_pixels col_pixels filters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size row_pixels col_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts numpy array of dimension: \u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    (batch_size, height, width, filters), defining an RGB image to a greyscale \u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    image of dimension: (batch_size, height, width)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m        np.ndarray: Greyscale version of the input image\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\comp0188_cw2\\config.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m     ROOT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/all_play_data_diverse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ROOT_PATH):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to create the file path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m FILE_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_play_data_diverse.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m _train_dh \u001b[38;5;241m=\u001b[39m DirectoryHandler(\n\u001b[0;32m     18\u001b[0m     loc\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_PATH,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     )\n",
      "\u001b[1;31mException\u001b[0m: You need to create the file path: ../data/all_play_data_diverse"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from pymlrf.Structs.torch import DatasetOutput\n",
    "import copy\n",
    "\n",
    "from comp0188_cw2.utils import load_all_files\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from typing import List\n",
    "from pymlrf.Structs.torch import DatasetOutput\n",
    "\n",
    "from comp0188_cw2.config import (\n",
    "    train_dh, val_dh, test_dh, WANDB_PROJECT\n",
    "    )\n",
    "from comp0188_cw2.models.CNNConfig import ConvolutionLayersConfig\n",
    "from comp0188_cw2.models.base import BaseModel\n",
    "from comp0188_cw2.models.JointCNNEncoder import JointCNNEncoder\n",
    "from comp0188_cw2.models.CNN import CNN\n",
    "from comp0188_cw2.models.MLP import MLP\n",
    "from comp0188_cw2.Metric.WandBMetricOrchestrator import WandBMetricOrchestrator\n",
    "from comp0188_cw2.Dataset.NpDictDataset import NpDictDataset\n",
    "from comp0188_cw2.Loss.BalancedLoss import TrackerBalancedLoss\n",
    "from comp0188_cw2 import logger\n",
    "from comp0188_cw2.training.TrainingLoop import TorchTrainingLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dh.loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "You will only need to perform this step __once__ for the full dataset and __once__ for the debug dataset for the entire coursework, both locally and in Google collab. In Google Collab, the data will be saved in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comp0188_cw2.Preprocessing import main\n",
    "RUN_PREPROCESSING = False\n",
    "if RUN_PREPROCESSING:\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Tune an end-to-end supervised learning model taking in the full set of observations and predicting actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_keys = [\n",
    "    \"actions\",\"front_cam_ob\",\n",
    "    \"mount_cam_ob\",\"terminals\",\n",
    "    \"ee_cartesian_pos_ob\",\n",
    "    \"ee_cartesian_vel_ob\",\n",
    "    \"joint_pos_ob\"\n",
    "    ]\n",
    "train_nps = load_all_files(train_dh.loc,\"train_[0-9]+.h5\",keys=_keys)\n",
    "val_nps = load_all_files(val_dh.loc,\"val_[0-9]+.h5\",keys=_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a Preprocessing\n",
    "Most likely in machine learning pipelines, input data needs to be preprocessed before passing it to the model. This question requires you to specify the preprocessing that you will perform for the different types of data i.e., front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob. The dataset class provided in the associated \"comp0188_cw2\" package enables you to pass a dictionary of functions to preprocess each element of the observations and actions. The class expects a dictionary of transformations to apply to each input/output. \n",
    "\n",
    "#### Question 1.a.i Preprocessing steps (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Complete the dictionaries below, specifying the type of transformations you wish to perform. For each element (of the observations and actions), you should __at least__ convert the output to a tensor thus, these transformations have been implemented for you. You may alter __any__ part of the code between the \"INSERT YOUR CODE HERE\" comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "global_transforms = {\n",
    "    \"front_cam_ob\":\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ]),\n",
    "    \"mount_cam_ob\": transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ]),\n",
    "    \"actions\": lambda x: torch.tensor(x),\n",
    "    \"ee_cartesian_pos_ob\": lambda x: torch.tensor(x),\n",
    "    \"ee_cartesian_vel_ob\": lambda x: torch.tensor(x),\n",
    "    \"joint_pos_ob\": lambda x: torch.tensor(x)\n",
    "}\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.a.ii Justification (5 marks)\n",
    "##### Task\n",
    "- In the markdown cell below, justify your decisions for preprocessing including where you have decided __not__ to apply preprocessing. You should include empirical evidence from your EDA analysis to support your decisions. Justfication __without__ evidence will be rewarded 0 marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_actions_\n",
    "\n",
    "\n",
    "_front_cam_ob_\n",
    "\n",
    "\n",
    "_mount_cam_ob_\n",
    "\n",
    "\n",
    "_ee_cartesian_pos_ob_\n",
    "\n",
    "\n",
    "_ee_cartesian_vel_ob_\n",
    "\n",
    "\n",
    "_joint_pos_ob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b End-to-end supervised model evaluation\n",
    "The code below defines an end to end supervised model which: \n",
    "- Jointly encodes the two images (\"front_cam_ob\", \"mount_cam_ob\") using a CNN architecture (image_encoder);\n",
    "- Seperately encoding the positional and velocity observations using an MLP;\n",
    "- Combines the two embeddings by adding them together and;\n",
    "- Passes the combined embedding into a final MLP layer (dense)\n",
    "  \n",
    "This question requires you to define sutable loss functions for the model and then evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2_config = ConvolutionLayersConfig(\n",
    "    input_dim=224,\n",
    "    input_channels=2,\n",
    "    layers=[\n",
    "        nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            dilation=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_cnn_output_dim = cnn2_config.get_output_dims()\n",
    "_cnn_output_channels = cnn2_config.get_output_channels()\n",
    "_mlp_input_dim = int(\n",
    "            (_cnn_output_dim[-1]*_cnn_output_dim[-1])*_cnn_output_channels[-1]\n",
    "            )\n",
    "\n",
    "\n",
    "image_encoder = JointCNNEncoder(\n",
    "    cnn=CNN(cnn2_config),\n",
    "    dense=MLP(\n",
    "        input_dim=_mlp_input_dim,\n",
    "        hidden_dims=[256],\n",
    "        output_dim=128\n",
    "        )\n",
    ")\n",
    "\n",
    "obs_encoder = MLP(\n",
    "    input_dim = 15,\n",
    "    hidden_dims = [256,256],\n",
    "    output_dim = 128\n",
    ")\n",
    "\n",
    "dense = MLP(\n",
    "    input_dim = 128,\n",
    "    hidden_dims = [64,32],\n",
    "    output_dim = 6\n",
    ")\n",
    "\n",
    "class Baseline1(BaseModel):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      image_encoder:JointCNNEncoder,\n",
    "      obs_encoder:MLP,\n",
    "      dense:MLP\n",
    "      ) -> None:\n",
    "      super().__init__()\n",
    "      self.image_encoder = image_encoder\n",
    "      self.obs_encoder = obs_encoder\n",
    "      self.dense = dense\n",
    "\n",
    "  def forward(self, images, obs):\n",
    "    _img_enc = self.image_encoder(images)\n",
    "    _obs_enc = self.obs_encoder(obs)\n",
    "    _dense_enc = self.dense(_img_enc+_obs_enc)\n",
    "    pos = _dense_enc[:,0:3]\n",
    "    grp = _dense_enc[:,3:]\n",
    "    return {\n",
    "        \"pos\": pos,\n",
    "        \"grp\":grp\n",
    "        }\n",
    "  def reset(\n",
    "      self,\n",
    "      image_encoder_kwargs,\n",
    "      obs_encoder_kwargs,\n",
    "      dense_kwargs\n",
    "      ):\n",
    "    self.image_encoder.reset(**image_encoder_kwargs)\n",
    "    self.obs_encoder.reset(**obs_encoder_kwargs)\n",
    "    self.dense.reset(**dense_kwargs)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = Baseline1(\n",
    "    image_encoder=image_encoder,\n",
    "    obs_encoder=obs_encoder,\n",
    "    dense=dense\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.b.i Loss definitions (3 marks)\n",
    "For the model defined above, the proposed loss function is defined where the contribution of \"pos_criterion\" and \"grp_criterion\" are equally weighted and the mean of the two values loss are used to define the final loss. Furthermore, the loss for the positional actions is the MSE and the loss for grp_criterion is the CrossEntropyLoss.\n",
    "\n",
    "##### Task:\n",
    "- Justify why this composite loss function is reasonable. You should make reference to the range of values predicted by the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_criterion = nn.MSELoss(reduction=\"mean\")\n",
    "grp_criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "model.reset({},{},{})\n",
    "exp_kwargs = {\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"half_precision\": True,\n",
    "    \"target_offset\": 0\n",
    "}\n",
    "\n",
    "if project_options.debug:\n",
    "    exp_kwargs[\"batch_size\"] = 4\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    train_dataset = NpDictDataset(\n",
    "        array_dict=train_nps,\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "    val_dataset = NpDictDataset(\n",
    "        array_dict=val_nps,\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "else:\n",
    "    train_dataset = NpDictDataset(\n",
    "        array_dict={k:train_nps[k].astype(np.float32) for k in train_nps},\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "    val_dataset = NpDictDataset(\n",
    "        array_dict={k:val_nps[k].astype(np.float32) for k in val_nps},\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "print(len(train_dataset))\n",
    "out = train_dataset[0]\n",
    "\n",
    "def collate_func(input_list:List[DatasetOutput])->DatasetOutput:\n",
    "    pos = []\n",
    "    _grp = []\n",
    "    images = []\n",
    "    obs = []\n",
    "    for val in input_list:\n",
    "        images.append(\n",
    "            torch.concat(\n",
    "                [val.input[\"front_cam_ob\"], val.input[\"mount_cam_ob\"]],\n",
    "                dim=0\n",
    "            )[None,:]\n",
    "            )\n",
    "        obs.append(\n",
    "            torch.concat(\n",
    "                [\n",
    "                    val.input[\"ee_cartesian_pos_ob\"],\n",
    "                    val.input[\"ee_cartesian_vel_ob\"],\n",
    "                    val.input[\"joint_pos_ob\"]\n",
    "                    ],\n",
    "                dim=0\n",
    "            )[None,:]\n",
    "        )\n",
    "        pos.append(val.output[\"actions\"][0:3][None,:])\n",
    "        _grp.append(val.output[\"actions\"][-1:][None])\n",
    "    _grp = torch.concat(_grp, dim=0)\n",
    "    grp = torch.zeros(_grp.shape[0],3)\n",
    "    grp[torch.arange(len(grp)), _grp.squeeze().int()] = 1\n",
    "    return DatasetOutput(\n",
    "        input = {\n",
    "            \"images\":torch.concat(images,dim=0),\n",
    "            \"obs\":torch.concat(obs,dim=0),\n",
    "            },\n",
    "        output = {\n",
    "            \"pos\":torch.concat(pos, dim=0),\n",
    "            \"grp\":grp\n",
    "            }\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=exp_kwargs[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_func,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=exp_kwargs[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_func,\n",
    ")\n",
    "\n",
    "first_batch = next(train_dataloader.__iter__())\n",
    "\n",
    "input_dim = first_batch.input[\"images\"].shape\n",
    "print(input_dim)\n",
    "input_dim = first_batch.input[\"obs\"].shape\n",
    "print(input_dim)\n",
    "pos_dim = first_batch.output[\"pos\"].shape\n",
    "print(pos_dim)\n",
    "grp_dim = first_batch.output[\"grp\"].shape\n",
    "print(grp_dim)\n",
    "\n",
    "exp_kwargs[\"model_def\"] = model.__repr__()\n",
    "\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    model = model.half()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=exp_kwargs[\"learning_rate\"],\n",
    "    eps=1e-04\n",
    "    )\n",
    "\n",
    "__criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":pos_criterion,\n",
    "        \"grp\": grp_criterion\n",
    "    }\n",
    "    )\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                res = model(\n",
    "                    images=first_batch.input[\"images\"].cuda(),\n",
    "                    obs=first_batch.input[\"obs\"].cuda()\n",
    "                    )\n",
    "                first_batch.output[\"pos\"] = first_batch.output[\"pos\"].cuda()\n",
    "                first_batch.output[\"grp\"] = first_batch.output[\"grp\"].cuda()\n",
    "        else:\n",
    "            with torch.autocast(device_type=\"cpu\"):\n",
    "                res = model(\n",
    "                    images=first_batch.input[\"images\"],\n",
    "                    obs=first_batch.input[\"obs\"]\n",
    "                    )\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            res = model(\n",
    "                images=first_batch.input[\"images\"].cuda(),\n",
    "                obs=first_batch.input[\"obs\"].cuda()\n",
    "                )\n",
    "            first_batch.output[\"pos\"] = first_batch.output[\"pos\"].cuda()\n",
    "            first_batch.output[\"grp\"] = first_batch.output[\"grp\"].cuda()\n",
    "        else:\n",
    "            res = model(\n",
    "                images=first_batch.input[\"images\"],\n",
    "                obs=first_batch.input[\"obs\"]\n",
    "                )\n",
    "print(__criterion(res,first_batch.output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = WandBMetricOrchestrator()\n",
    "\n",
    "train_criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":copy.deepcopy(pos_criterion),\n",
    "        \"grp\":copy.deepcopy(grp_criterion)\n",
    "    },\n",
    "    name=\"train\",\n",
    "    mo=mo\n",
    "    )\n",
    "\n",
    "val_criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":copy.deepcopy(pos_criterion),\n",
    "        \"grp\":copy.deepcopy(grp_criterion)\n",
    "    },\n",
    "    name=\"val\",\n",
    "    mo=mo\n",
    "    )\n",
    "\n",
    "sl_trainer = TorchTrainingLoop(\n",
    "    model=model, gpu=True, optimizer=optimizer, criterion=train_criterion,\n",
    "    val_criterion=val_criterion, epochs=10, logger=logger,\n",
    "    mo=WandBMetricOrchestrator(), half_precision=exp_kwargs[\"half_precision\"],\n",
    "    preds_save_type=None\n",
    ")\n",
    "\n",
    "wandb_name = \"End_to_end_1\"\n",
    "wandb_grp=\"End_to_end\"\n",
    "\n",
    "if project_options.debug:\n",
    "    wandb_name = f\"{wandb_name}_DEBUG\"\n",
    "\n",
    "\n",
    "orig = datetime.datetime.now()\n",
    "sl_trainer.training_loop(\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    wandb_proj=WANDB_PROJECT,\n",
    "    wandb_grp=wandb_grp,\n",
    "    wandb_config=exp_kwargs,\n",
    "    wandb_name=wandb_name,\n",
    "    reset_kwargs={\n",
    "        \"image_encoder_kwargs\": {},\n",
    "        \"obs_encoder_kwargs\": {},\n",
    "        \"dense_kwargs\": {}\n",
    "    }\n",
    "    )\n",
    "post_train = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.b.ii Model evaluation (marks broken down in sub questions)\n",
    "This question requires you to evaluate the perfomance of the model defined above by interpreting model training/validation metrics and investigating the specific failure modes of the model. Run the cells above which will train the model for 10 epochs. \n",
    "\n",
    "**IMPORTANT, for questions (1.b.ii.i, 1.b.ii.ii, 1.b.ii.ii)** \n",
    "- You are **not** expected to train a baseline model and you will be awarded **0 marks** for training such a model and evaluating performance against it. \n",
    "- Furthermore, when interpretting the metric(s) and justifying your conclusions, you will loose marks for failing to comment on striking results that are obvious to the marker.\n",
    "- **Incorrect interpretations of metrics** will be **negatively penalised**\n",
    "- Finally, where you have been asked for empirical evidence, but provide none, you will be awarded __0 marks__.\n",
    "\n",
    "#### Question 1.b.ii.i Epoch selection (3 marks)\n",
    "##### Task\n",
    "- Select an appropriate epoch using a suitable method, provide __empirical evidence__ for your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.b.ii.ii Performance evaluation (8 marks)\n",
    "##### Task\n",
    "- Draw conclusions regarding how well the model performs. To do this you may want to use additional metrics to the one(s) already reported. Critically, you should __contextualise__ the performance of the model. We are expecting discussion points a long the lines: _The model achieved a score of X according to metric Y. This suggests that the model is/is not performing well, because..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.b.ii.iii Model failure modes (9 marks)\n",
    "##### Task\n",
    "- Investigate the failure modes of the model. You should: \n",
    "  - Demonstrate the failure mode using **empirical evidence** e.g., _the model fails on X observations as demonstrated by..._\n",
    "  - Assess the impact of these failure modes using **empirical evidence** i.e., how significant is this problem. You should consider both metrics and dataset coverage\n",
    "  - Conclude __why__ the model might be failing on these observations **empirical evidence** e.g., _X part of the model performs poorly, as demonstrated by..._\n",
    "\n",
    "__IMPORTANT__:\n",
    "- Marks will be penalised if the failure modes you identify are not systematic i.e., you select a single observation that the model fails on. If you cannot identify any general failure modes of the model, you will be awarded full marks if you provide empirical evidence justifying why there are no systematic failure modes which sufficienty accounts for the diversity of observations in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.c Model tuning\n",
    "Now you have evaluated the proposed model, you are required to iterate and train a higher performing one. You are expected to run experiments that help you understand where the model is underperforming, guiding your development.\n",
    "\n",
    "#### Question 1.c.i Model tuning (5 marks)\n",
    "##### Task\n",
    "- Using the code blocks below, implement a model which improves over the previous. Improve the performance as best you can and report the results using the metric/metrics you used in question 1.b.ii Model evaluation. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.c.ii Discussion (marks broken down in subsections below)\n",
    "\n",
    "##### Task\n",
    "In the markdown blocks below, discuss **three** experiments that you ran during the development of your model, defined in 1.c.i Model tuning which were the **most insightful** with respect to the design of your final model. Importantly, **most insightful** needn't necessarily be the final decisions that appear in the model defined in question 1.c.i Model tuning, rather they should be experiments that most shaped your model development process. For example: \n",
    "- You might decide to use a learning rate scheduler and decrease the learning rate at epoch 10 (call this experiment \"EXP_LR\");\n",
    "- This experiment produces a jump in performance and unlocks a series of further fruitful experiments into learning rate scheduling;\n",
    "- However, in the model reported in question 1.c.i Model tuning, you use a learning rate schedule with descreases at epochs 6 and 15 (since these produced a marginal performance over \"EXP_LR\")\n",
    "- For this question __you should discuss EXP_LR__.\n",
    "  \n",
    "For each experiment, complete the \"Description\", \"Result\" and \"Conclusion\" sections where the following information should be provided:\n",
    "* __Description__: What delta were you measuring i.e., change of architecture, change of learning rate etc?\n",
    "* __Justification__: **Why** are you conducting the experiment?\n",
    "  * What was the context of the model development process up to this point? What did you already know about how well the model performed/why it was performing/what were the challenging data points?\n",
    "* __Conclusion__: What did you __learn__ from the experiment and provide __empirical evidence__ to support this claim. In drawing your conclusions, consider where there are multiple possible causes for the model failing, ensure you provide evidence for each of these and conclude whether or not they might be root cause (you may conclude that there are multiple causes).\n",
    "\n",
    "**IMPORTANT** If your reported experiments are **not** well motivated and do not demonstrate that you have integrogated the model performance, you will be deducted marks. An example of poor motovation might be: *I chose to decrease the learning rate from X to Y as I did not know what impact this might have*. This justification could be improved by explaining what you __already know__ about how the learning rate might be affecting the model. What hypotheses might you draw about what the experiment will show?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.c.ii.i Experiment 1 discussion (10 marks)\n",
    "\n",
    "##### Description\n",
    "\n",
    "\n",
    "##### Justification\n",
    "\n",
    "\n",
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.c.ii.ii Experiment 2 discussion (10 marks)\n",
    "\n",
    "##### Description\n",
    "\n",
    "\n",
    "##### Justification\n",
    "\n",
    "\n",
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.c.ii.iii Experiment 3 discussion (10 marks)\n",
    "\n",
    "##### Description\n",
    "\n",
    "\n",
    "##### Justification\n",
    "\n",
    "\n",
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 Self-supervised pretraining with VAEs\n",
    "\n",
    "This question requires you to implement a self-supervised approach using a VAE architecture. The focus of question 2 is in developing a VAE model without using __any__ supervised information i.e., without using any action information. You will assess the convergence of the model and the suitability of the learnt latent space. You are required to select and appropriate architecture/loss function/target set of observations to perform self-supervised learning over.\n",
    "\n",
    "__IMPORTANT__: Do not use any of the __action__ information. You will be awarded 0 marks if you do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a Self-supervised VAE model (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Implement the full model training process and model definiton in the code block below. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.b Model convergence (12 marks)\n",
    "\n",
    "##### Task\n",
    "- Provide empirical evidence (in the form of appropriate training/validation metrics) supporting **why** the model is appropriately trained;\n",
    "- Interpret why the metrics demonstrate that the model has converged. If your model has **not** converged, interpret why the metrics suggest so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c Latent space analysis (6 marks)\n",
    "\n",
    "##### Task\n",
    "- Using reasonable analysis, conclude whether the representation learnt by the self-supervised method will be beneficial for the downstream supervised task. **Empirical evidence** should be provided **however**, references to the performance of the self-superised method with a supervised head on the downstream task will be awarded __0 marks__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Full model training\n",
    "This question requires you to first develop a supervised head which utilises the latent space from your self-supervised method. You are then required to assess the models performance as well as the model you developed in question 1.c.i on the test set, report the performance and conclude whether self-supervised learning is appropriate for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a Combining self-supervised model with the downstream task (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Develop a model which combines the self-supervised pretraining with a model for performing the downstream task by freezing the self-supervised model and fine-tuning a head for prediction and implement it in the code block below. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions). The supervised head should at least include any inputs that you did not feed into the self-supervised model. For example, assume you decide to perform self-supervised learning only using front_cam_ob images. You must also include mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob observations in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.b Assessing the suitability of self-supervised learning \n",
    "For the final two questions, you are required to assess the performance of the self-supervised + supervised head model and the end-to-end model that you have trained. Additionally, you are required to holistically evaluate whether self-supervised learning has been beneficial for this task.\n",
    "\n",
    "#### Question 3.b.i Assessing the suitability of self-supervised learning (4 marks) \n",
    "\n",
    "##### Task\n",
    "- In the code block below, evaluate the performance of the model you trained in question 3.a and the model you trained in question 1.c.i, using the test set. Additionally use the same metrics to train and evaluate the model that you used for question 1.c.i. Markers should be able to run the cell such that both models are run on the appropriate dataset. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.b.ii Justification (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Conclude whether the self-supervised pre-training was beneficial for the task of predicting actions. Your answer should not solely focus on final performance but rather be nuianced and balance other model development considerations for example parameter count and speed of convergence. Also, if you believe the comparison between the model trained in question 3.a.i aganst the model trained in question 2.c.i is not _fair_, discuss further experiments which you would perform to reduce the bias in your conclusions. Provide __empirical evidence__ to support your conclusions. __0 marks__ will be awarded if empirical evidence is __not__ provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
