{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comp0188_cw2 import project_options\n",
    "project_options.collab = False\n",
    "project_options.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comp0188_cw2.utils import load_all_files\n",
    "from comp0188_cw2.Dataset.NpDictDataset import NpDictDataset\n",
    "from comp0188_cw2.config import WANDB_PROJECT, val_dh, train_dh\n",
    "print(val_dh.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_keys = [\n",
    "    \"actions\",\"front_cam_ob\",\n",
    "    \"mount_cam_ob\",\"terminals\",\n",
    "    \"ee_cartesian_pos_ob\",\n",
    "    \"ee_cartesian_vel_ob\",\n",
    "    \"joint_pos_ob\"\n",
    "    ]\n",
    "train_nps = load_all_files(train_dh.loc,\"train_[0-9]+.h5\",keys=_keys)\n",
    "val_nps = load_all_files(val_dh.loc,\"val_[0-9]+.h5\",keys=_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comp0188_cw2.models.CNNConfig import ConvolutionLayersConfig\n",
    "from comp0188_cw2.models.CNN import CNN\n",
    "from comp0188_cw2.models.MLP import MLP\n",
    "from comp0188_cw2.models.JointCNNEncoder import JointCNNEncoder\n",
    "from comp0188_cw2.models.base import BaseModel\n",
    "\n",
    "cnn2_config = ConvolutionLayersConfig(\n",
    "    input_dim=224,\n",
    "    input_channels=2,\n",
    "    layers=[\n",
    "        nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            dilation=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_cnn_output_dim = cnn2_config.get_output_dims()\n",
    "_cnn_output_channels = cnn2_config.get_output_channels()\n",
    "_mlp_input_dim = int(\n",
    "            (_cnn_output_dim[-1]*_cnn_output_dim[-1])*_cnn_output_channels[-1]\n",
    "            )\n",
    "\n",
    "\n",
    "image_encoder = JointCNNEncoder(\n",
    "    cnn=CNN(cnn2_config),\n",
    "    dense=MLP(\n",
    "        input_dim=_mlp_input_dim,\n",
    "        hidden_dims=[256],\n",
    "        output_dim=128\n",
    "        )\n",
    ")\n",
    "\n",
    "obs_encoder = MLP(\n",
    "    input_dim = 15,\n",
    "    hidden_dims = [256,256],\n",
    "    output_dim = 128\n",
    ")\n",
    "\n",
    "dense = MLP(\n",
    "    input_dim = 128,\n",
    "    hidden_dims = [64,32],\n",
    "    output_dim = 6\n",
    ")\n",
    "\n",
    "class Baseline1(BaseModel):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      image_encoder:JointCNNEncoder,\n",
    "      obs_encoder:MLP,\n",
    "      dense:MLP\n",
    "      ) -> None:\n",
    "      super().__init__()\n",
    "      self.image_encoder = image_encoder\n",
    "      self.obs_encoder = obs_encoder\n",
    "      self.dense = dense\n",
    "\n",
    "  def forward(self, images, obs):\n",
    "    _img_enc = self.image_encoder(images)\n",
    "    _obs_enc = self.obs_encoder(obs)\n",
    "    _dense_enc = self.dense(_img_enc+_obs_enc)\n",
    "    pos = _dense_enc[:,0:3]\n",
    "    grp = _dense_enc[:,3:]\n",
    "    return {\n",
    "        \"pos\": pos,\n",
    "        \"grp\":grp\n",
    "        }\n",
    "  def reset(\n",
    "      self,\n",
    "      image_encoder_kwargs,\n",
    "      obs_encoder_kwargs,\n",
    "      dense_kwargs\n",
    "      ):\n",
    "    self.image_encoder.reset(**image_encoder_kwargs)\n",
    "    self.obs_encoder.reset(**obs_encoder_kwargs)\n",
    "    self.dense.reset(**dense_kwargs)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = Baseline1(\n",
    "    image_encoder=image_encoder,\n",
    "    obs_encoder=obs_encoder,\n",
    "    dense=dense\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(\"jsphd/cw2_v2/End_to_end_1-mdl_chkpnt_epoch_6.pt:v0\")\n",
    "artifact_dir = artifact.download()\n",
    "#mdl_checkpoint = torch.load(artifact_dir)\n",
    "model.load_state_dict(\n",
    "    torch.load(os.path.join(artifact_dir,\"mdl_chkpnt_epoch_6.pt\"), \n",
    "               map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
