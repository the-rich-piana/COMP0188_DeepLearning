{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP0188 RNN tutorial\n",
    "\n",
    "This tutorial discusses how to develop RNN's in PyTorch, in particular RNNs for time series prediction. The following topics are covered:\n",
    "* Tensor structure for sequences\n",
    "* Vanilla RNN model in Pytorch\n",
    "* Stacked RNNs\n",
    "* Predicting n-day values\n",
    "\n",
    "The data used in the tutorial can be downloaded from: https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
    "\n",
    "__NOTE__: The code in this tutorial is sometimes unecessarily verbose to expose students to coding practices that are often helpful in developing machine learning pipelines. Where this is the case, the code will be marked with \"# verbose code\"\n",
    "\n",
    "Connect environment to a GPU by:\n",
    "* Select 'Runtime' in the top left\n",
    "* Select 'Change Runtime Type'\n",
    "* Select the GPU runtime available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from typing import Union, Callable, Tuple, List, Literal, Dict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "from abc import ABCMeta, abstractmethod\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tensor Structure for Sequences\n",
    "When working with sequences, such as time series or sentences, RNNs require input data to be formatted as tensors with specific dimensions. Understanding these tensor dimensions is crucial for developing RNN models.\n",
    "\n",
    "#### Tensor Dimensions in RNNs\n",
    "RNNs in PyTorch expect input tensors in a specific format. For a batch of sequences, the input tensor dimensions are:\n",
    "\n",
    "* Batch Size (`N`)**: The number of sequences in a batch.\n",
    "* Sequence Length (`T`)**: The length of each sequence (e.g., number of time steps in a time series).\n",
    "* Feature Size (`F`)**: The dimensionality of each input element (e.g., the number of features for each time step).\n",
    "\n",
    "Thus, the input tensor shape is `(N, T, F)` when `batch_first=True`. Otherwise, it is `(T, N, F)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example: Batch size of 2, sequence length of 5, feature size of 3\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "feature_size = 3\n",
    "\n",
    "# Create a random tensor with shape (batch_size, seq_length, feature_size)\n",
    "input_tensor = torch.randn(batch_size, seq_length, feature_size)\n",
    "print(\"Input Tensor Shape:\", input_tensor.shape)  # Output: (2, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flow of Tensors Through RNN Models\n",
    "* Input to RNN Layer: The input tensor of shape (N, T, F) is fed into the RNN layer. The RNN processes each element in the sequence one time step at a time, using its internal hidden state, which is updated after processing each element.\n",
    "* Hidden State Initialization: At the beginning of the sequence processing, the hidden state is initialized (usually to zeros). The shape of the hidden state tensor is (num_layers * num_directions, N, hidden_size), where:\n",
    "  * num_layers is the number of RNN layers.\n",
    "  * num_directions is 2 if the RNN is bidirectional; otherwise, it is 1.\n",
    "  * hidden_size is the number of features in the hidden state.\n",
    "* Output of RNN Layer:\n",
    "  * Output Tensor: Shape (N, T, hidden_size) if batch_first=True. It contains the hidden states from the last RNN layer for each time step in the sequence.\n",
    "  * Hidden State Tensor: Shape (num_layers * num_directions, N, hidden_size), which contains the hidden state for the last time step of each layer. The number of hidden state is a hyperparameter.\n",
    "* Output to Fully Connected Layer:\n",
    "  * In many applications, we only care about the output at the final time step. Therefore, we often take the output tensor from the last time step (out[:, -1, :] if batch_first=True) and pass it to a fully connected layer to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([2, 5, 4])\n",
      "Hidden State Shape: torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# RNN configuration\n",
    "input_size = feature_size  # Number of input features per time step\n",
    "hidden_size = 4  # Number of features in the hidden state\n",
    "num_layers = 1  # Number of stacked RNN layers\n",
    "\n",
    "# Define the RNN layer\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "# Initialize hidden state\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "output, hn = rnn(input_tensor, h0)\n",
    "print(\"Output Tensor Shape:\", output.shape)  # Output: (2, 5, 4)\n",
    "print(\"Hidden State Shape:\", hn.shape)  # Output: (1, 2, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RNNs with Differing Sequence Length Inputs\n",
    "RNNs expect inputs to be in batches with a consistent sequence length for each batch. However, in many real-world scenarios, sequences have differing lengths. For instance:\n",
    "\n",
    "* In text processing, sentences can have different numbers of words.\n",
    "* In time-series data, different samples may have varying numbers of time steps.\n",
    "Handling these varying lengths in RNNs requires special techniques to ensure that shorter sequences are correctly processed without losing their shorter time-step information.\n",
    "\n",
    "##### Approaches to Handling Varying Sequence Lengths in RNNs\n",
    "* Padding Sequences: The most common approach to dealing with varying sequence lengths is to pad the shorter sequences so that all sequences in a batch have the same length. Padding is achieved by adding dummy values (typically zeros) to the end of shorter sequences. The padded sequences can then be processed by the RNN, but the padding itself should not affect the learning process. We can address this by using masking or packing techniques.\n",
    "* Packed sequences allow the RNN to efficiently ignore the padding by only processing the actual (non-padded) sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences:\n",
      " tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[10, 11, 12],\n",
      "         [13, 14, 15],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[16, 17, 18],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24],\n",
      "         [25, 26, 27]]])\n",
      "Packed output shape: torch.Size([3, 4, 4])\n",
      "Hidden state shape: torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Example input sequences with different lengths, each time step has 3 features\n",
    "# The sequence can have a variable \n",
    "sequences = [\n",
    "    torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),  # length 3 (3 time steps, each with 3 features)\n",
    "    torch.tensor([[10, 11, 12], [13, 14, 15]]),       # length 2\n",
    "    torch.tensor([[16, 17, 18], [19, 20, 21], [22, 23, 24], [25, 26, 27]])  # length 4\n",
    "]\n",
    "\n",
    "# Step 1: Pad the sequences to the same length\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "# Example sequence lengths\n",
    "lengths = torch.tensor([3, 2, 4])\n",
    "\n",
    "print(\"Padded sequences:\\n\", padded_sequences)\n",
    "\n",
    "# Step 2: Define RNN\n",
    "input_size = 3  # Number of features in each time step\n",
    "hidden_size = 4  # Number of features in the hidden state\n",
    "batch_size = len(sequences)\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "# Since the input_size is 1, we need to add an extra dimension to the input\n",
    "padded_sequences = padded_sequences.float()\n",
    "\n",
    "# Step 3: Pack the padded sequences\n",
    "packed_input = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "# Step 4: Pass through RNN\n",
    "h0 = torch.zeros(1, batch_size, hidden_size).float()  # Initialize hidden state as float\n",
    "packed_output, hn = rnn(packed_input, h0)\n",
    "\n",
    "# Step 5: Unpack the output\n",
    "output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "print(\"Packed output shape:\", output.shape)  # Shape: (batch_size, max_seq_len, hidden_size)\n",
    "print(\"Hidden state shape:\", hn.shape)        # Shape: (1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the above example code\n",
    "* Input Tensor: The input tensor is of shape (2, 5, 3), which matches the required (N, T, F) format.\n",
    "* RNN Output: The output tensor has the shape (2, 5, 4), which means for each of the 2 sequences (batch size), there are 5 time steps, and the hidden state has 4 features.\n",
    "* Hidden State (hn): The hidden state tensor at the last time step has the shape (1, 2, 4), representing 1 layer, 2 sequences, and a hidden state size of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/cs/academic/phd3/xinrzhen/xinran/TA'\n",
    "# logger = logging.getLogger(\"rnn_tutorial\")\n",
    "# logger.setLevel(logging.INFO)\n",
    "# WANDB_PROJ = \"rnn_tutorial\"\n",
    "# TMP_DIR = \"./tmp\"\n",
    "\n",
    "# if os.path.isdir(TMP_DIR):\n",
    "#     pass\n",
    "# else:\n",
    "#     os.mkdir(TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset\n",
    "The double for loop in the code block below (also copied directly) below defines the train/test data structure (mentioned above):\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | \n",
    "\n",
    "* Generate datasets for different steps\n",
    "  * Setting a step causes columns with a step greater than 1 to be shorter than the original DataFrame. This is because the shift(-step) function shifts the data up a step row, which causes the last few rows of the data to become NaN when the step is greater than 1. \n",
    "  * These NaN values are usually discarded when dealing with prediction problems, reducing the amount of valid data.\n",
    "```python\n",
    "steps = [1,5,10]\n",
    "for df in [train_df, test_df]:\n",
    "    for stp in steps:\n",
    "        df[[f\"{col}_{stp}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-1*stp)\n",
    "```\n",
    "By resetting dataset following differents steps, \n",
    "\n",
    "However, it is generalised in the following ways:\n",
    "* To compute not just the meantemp at t+1 but all features at time t+1;\n",
    "* To compute t+step target values, not just t+1\n",
    "\n",
    "This generalisation is useful for performing __Exercise 6b__ and __Exercise 8__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cs/academic/phd3/xinrzhen/xinran/TA\\\\DailyDelhiClimateTrain.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDailyDelhiClimateTrain.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDailyDelhiClimateTest.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Gugu\\Desktop\\UCL\\Deep_Learning\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cs/academic/phd3/xinrzhen/xinran/TA\\\\DailyDelhiClimateTrain.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTrain.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTest.csv\"))\n",
    "\n",
    "train_df[\"date\"] = pd.to_datetime(train_df[\"date\"], format=\"%Y-%m-%d\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
      "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
      "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
      "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
      "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
      "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
      "\n",
      "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
      "0        92.000000           2.980000          1017.800000         7.000000   \n",
      "1        87.000000           4.633333          1018.666667         7.000000   \n",
      "2        71.333333           1.233333          1017.166667         8.857143   \n",
      "3        86.833333           3.700000          1016.500000        14.000000   \n",
      "4        82.800000           1.480000          1018.000000        11.000000   \n",
      "\n",
      "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
      "0        82.800000           1.480000          1018.000000         15.714286   \n",
      "1        78.600000           6.300000          1020.000000         14.000000   \n",
      "2        63.714286           7.142857          1018.714286         15.833333   \n",
      "3        51.250000          12.500000          1017.000000         12.833333   \n",
      "4        62.000000           7.400000          1015.666667         14.714286   \n",
      "\n",
      "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
      "0         51.285714           10.571429           1016.142857  \n",
      "1         74.000000           13.228571           1015.571429  \n",
      "2         75.166667            4.633333           1013.333333  \n",
      "3         88.166667            0.616667           1015.166667  \n",
      "4         71.857143            0.528571           1015.857143  \n",
      "           date   meantemp    humidity  wind_speed  meanpressure  \\\n",
      "1457 2016-12-28  17.217391   68.043478    3.547826   1015.565217   \n",
      "1458 2016-12-29  15.238095   87.857143    6.000000   1016.904762   \n",
      "1459 2016-12-30  14.095238   89.666667    6.266667   1017.904762   \n",
      "1460 2016-12-31  15.052632   87.000000    7.325000   1016.100000   \n",
      "1461 2017-01-01  10.000000  100.000000    0.000000   1016.000000   \n",
      "\n",
      "      meantemp_1_step  humidity_1_step  wind_speed_1_step  \\\n",
      "1457        15.238095        87.857143           6.000000   \n",
      "1458        14.095238        89.666667           6.266667   \n",
      "1459        15.052632        87.000000           7.325000   \n",
      "1460        10.000000       100.000000           0.000000   \n",
      "1461              NaN              NaN                NaN   \n",
      "\n",
      "      meanpressure_1_step  meantemp_5_step  humidity_5_step  \\\n",
      "1457          1016.904762              NaN              NaN   \n",
      "1458          1017.904762              NaN              NaN   \n",
      "1459          1016.100000              NaN              NaN   \n",
      "1460          1016.000000              NaN              NaN   \n",
      "1461                  NaN              NaN              NaN   \n",
      "\n",
      "      wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
      "1457                NaN                  NaN               NaN   \n",
      "1458                NaN                  NaN               NaN   \n",
      "1459                NaN                  NaN               NaN   \n",
      "1460                NaN                  NaN               NaN   \n",
      "1461                NaN                  NaN               NaN   \n",
      "\n",
      "      humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
      "1457               NaN                 NaN                   NaN  \n",
      "1458               NaN                 NaN                   NaN  \n",
      "1459               NaN                 NaN                   NaN  \n",
      "1460               NaN                 NaN                   NaN  \n",
      "1461               NaN                 NaN                   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset with multiple step predictions\n",
    "non_date_vars = [col for col in train_df.columns if col != \"date\"]  # Exclude date column\n",
    "steps = [1, 5, 10]  # Predict future values at different steps\n",
    "for df in [train_df, test_df]:\n",
    "    for step in steps:\n",
    "        df[[f\"{col}_{step}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-step)\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.tail())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test split\n",
    "* The training dataset also needs to be split into a training and holdout set. When using any data where observations are non iid, data must be split to prevent \"data leakage\". Time series data is likely non-iid in the sense that future observations most likely depend on previous ones for example, it is reasonable to assume that the meantemp at time t+1 is dependant on the meantemp at time t.\n",
    "* The dataset therefore needs to be split such that the ML models is not explosed to correlations which would not be available at test time.\n",
    "* Given this, the final year of the training data is used as the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1095, 17) (367, 17) (114, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
       "\n",
       "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
       "0        92.000000           2.980000          1017.800000         7.000000   \n",
       "1        87.000000           4.633333          1018.666667         7.000000   \n",
       "2        71.333333           1.233333          1017.166667         8.857143   \n",
       "3        86.833333           3.700000          1016.500000        14.000000   \n",
       "4        82.800000           1.480000          1018.000000        11.000000   \n",
       "\n",
       "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "0        82.800000           1.480000          1018.000000         15.714286   \n",
       "1        78.600000           6.300000          1020.000000         14.000000   \n",
       "2        63.714286           7.142857          1018.714286         15.833333   \n",
       "3        51.250000          12.500000          1017.000000         12.833333   \n",
       "4        62.000000           7.400000          1015.666667         14.714286   \n",
       "\n",
       "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "0         51.285714           10.571429           1016.142857  \n",
       "1         74.000000           13.228571           1015.571429  \n",
       "2         75.166667            4.633333           1013.333333  \n",
       "3         88.166667            0.616667           1015.166667  \n",
       "4         71.857143            0.528571           1015.857143  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and holdout sets to prevent data leakage\n",
    "train_df[\"__date_yrs\"] = train_df[\"date\"].dt.year\n",
    "val_idx = train_df[\"__date_yrs\"] >= 2016  # Holdout data for the last year\n",
    "val_df = train_df[val_idx].drop(columns=[col for col in train_df.columns if col.startswith(\"__\")])\n",
    "train_df = train_df[~val_idx].drop(columns=[col for col in train_df.columns if col.startswith(\"__\")])\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting next day values\n",
    "* For the first exercise, daily climate data will be used to develop a model that can predict the next day \"meantemp\".\n",
    "* First of all, training and test datasets need to be manipulated such that they are in the following form:\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the Vanilla RNN Model in PyTorch\n",
    "We will now implement a simple RNN model using PyTorch's nn.RNN module. This model will take sequences of historical data and predict future values.\n",
    "\n",
    "* Model Architecture\n",
    "  * RNN Layer: The core component of the model that processes sequences.\n",
    "  * Fully Connected Layer: A linear layer that maps the hidden states produced by the RNN to the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose code\n",
    "# Here a series of Debug classes are defined. The reason for using this structure will be discussed in class\n",
    "class DebugPass:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def debug(\n",
    "        self, \n",
    "        y_true:torch.tensor, \n",
    "        y_pred:torch.tensor, \n",
    "    ):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugBase(DebugPass):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def debug(self, y_true, y_pred):\n",
    "        self.y_true.append(y_true.cpu().detach().numpy())\n",
    "        self.y_pred.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugLocal(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        res_tbl.to_csv(os.path.join(TMP_DIR, \"validate_debug.csv\"), index=False)\n",
    "\n",
    "class DebugWandB(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        wandb_tbl = wandb.Table(dataframe=res_tbl)\n",
    "        wandb.log({\"val_predictions\" : wandb_tbl})\n",
    "\n",
    "def train_single_epoch(model:nn.Module, data_loader:torch.utils.data.DataLoader, \n",
    "                       gpu:Literal[True, False], optimizer:torch.optim,\n",
    "                       criterion:torch.nn.modules.loss\n",
    "                      ) -> Tuple[List[torch.Tensor]]:\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    preds = []\n",
    "    range_gen = tqdm(\n",
    "        enumerate(data_loader),\n",
    "        )\n",
    "    for i, (y,X) in range_gen:\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(X)\n",
    "        preds.append(output)\n",
    "        \n",
    "        train_loss = criterion(output, y)\n",
    "        losses.append(train_loss.item())\n",
    "\n",
    "        # losses.update(train_loss.data[0], g.size(0))\n",
    "        # error_ratio.update(evaluation(output, target).data[0], g.size(0))\n",
    "\n",
    "        try: \n",
    "            # compute gradient and do SGD step\n",
    "            train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(\"Runtime error on training instance: {}\".format(i))\n",
    "            raise e\n",
    "    return losses, preds\n",
    "\n",
    "def validate(model:nn.Module, data_loader:torch.utils.data.DataLoader,\n",
    "             gpu:Literal[True, False], criterion:torch.nn.modules.loss,\n",
    "             dh:DebugPass\n",
    "            ) -> Tuple[List[torch.Tensor]]:\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    losses = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        range_gen = tqdm(\n",
    "            enumerate(data_loader),\n",
    "        )\n",
    "        # Your code here\n",
    "        for i, (y,X) in range_gen:\n",
    "        \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(X)\n",
    "\n",
    "            # Logs\n",
    "            losses.append(criterion(output, y).item())\n",
    "            preds.append(output)\n",
    "            dh.debug(y_true=y, y_pred=output)\n",
    "    return losses, preds\n",
    "\n",
    "\n",
    "def train(model:torch.nn, train_data_loader:torch.utils.data.DataLoader,\n",
    "          val_data_loader:torch.utils.data.DataLoader, \n",
    "          gpu:Literal[True, False], optimizer:torch.optim,\n",
    "          criterion:torch.nn.modules.loss, epochs:int, \n",
    "          debug:bool = False, wandb_proj:str=\"\", \n",
    "          wandb_config:Dict={}\n",
    "         ) -> Tuple[List[torch.Tensor]]:\n",
    "\n",
    "    if (len(wandb_config) == 0) or (len(wandb_proj) == 0):\n",
    "        use_wandb = False\n",
    "        logger.warning(\"WandB not in use!\")\n",
    "        chkpnt_dir = TMP_DIR\n",
    "    else:\n",
    "        use_wandb = True\n",
    "        wandb.init(project=wandb_proj, config=wandb_config)\n",
    "        chkpnt_dir = wandb.run.dir\n",
    "\n",
    "    if debug:\n",
    "        if use_wandb:\n",
    "            dh = DebugWandB()\n",
    "        else:\n",
    "            dh = DebugLocal()\n",
    "    else:\n",
    "        dh = DebugPass()\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Running training epoch\")\n",
    "        train_loss_val, train_preds =  train_single_epoch(\n",
    "            model=model, data_loader=train_data_loader, gpu=gpu, \n",
    "            optimizer=optimizer, criterion=criterion)\n",
    "        mean_train_loss = np.mean(train_loss_val)\n",
    "        epoch_train_loss.append(mean_train_loss)\n",
    "        val_loss_val, val_preds = validate(\n",
    "            model=model, data_loader=val_data_loader, gpu=gpu, \n",
    "            criterion=criterion, dh=dh)\n",
    "        \n",
    "        print(\"Running validation\")\n",
    "        mean_val_loss = np.mean(val_loss_val)\n",
    "        epoch_val_loss.append(np.mean(val_loss_val))\n",
    "\n",
    "        chkp_pth = os.path.join(chkpnt_dir, f\"mdl_chkpnt_epoch_{epoch}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, chkp_pth)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train_loss\": mean_train_loss, \"val_loss\": mean_val_loss})\n",
    "            wandb.save(chkp_pth)\n",
    "    dh.close()\n",
    "    if use_wandb: \n",
    "        wandb.finish()\n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset class for time series data format\n",
    "* Using the PandasDataset class, data is batched according to the time dimension. This dataset object needs to be extended such that sequences of length longer than 1 can be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.Series, normalise:bool=True)->None:\n",
    "        self._X = torch.from_numpy(X.values).float()\n",
    "        if normalise:\n",
    "            self._X = self.__min_max_norm(self._X)\n",
    "        self.feature_dim = X.shape[1]\n",
    "        self._len = X.shape[0]\n",
    "        self._y = torch.from_numpy(y.values)[:,None].float()\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self._y[idx], self._X[idx,:]\n",
    "        \n",
    "    def __min_max_norm(self, in_tens:torch.Tensor) -> torch.Tensor:\n",
    "        # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        _min = in_tens.min(axis=0).values\n",
    "        _max = in_tens.max(axis=0).values\n",
    "        in_tens = (in_tens - _min)/(_max - _min)\n",
    "        return in_tens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 1 step mean temperature\n",
    "* For the first exercise, the aim is to predict the next day mean temperature using a historial time series of temperature, humidity, wind_speed and meanpressure values\n",
    "* A core hyperparameter is defining the sequence length i.e., the size of the historial time series to use for prediction.\n",
    "* As it stands, the data is of the form (time t obs, time t+1 target). Therefore, if this data was converted to a tensor as is, batched and used for training, only the time t values would be used for prediction. Using the \"PandasDataset\" class from the first tutorial demonstrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meantemp_1_step\n",
      "['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n"
     ]
    }
   ],
   "source": [
    "trgt_col = \"meantemp_1_step\" # This is the t+1 target associated with the time t observations\n",
    "# Dynamically select the remaining feature columns i.e., those that are: \n",
    "# 1) Not target variables (do not end with _step) and;\n",
    "# 2) Are not the date variable\n",
    "indp_cols = [ # verbose code\n",
    "    col for col in train_df.columns if (\n",
    "        (col != \"date\") and (col[-5:] != \"_step\")\n",
    "    )\n",
    "]\n",
    "print(trgt_col)\n",
    "print(indp_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    meantemp   humidity  wind_speed  meanpressure  meantemp_1_step\n",
       "0  10.000000  84.500000    0.000000   1015.666667         7.400000\n",
       "1   7.400000  92.000000    2.980000   1017.800000         7.166667\n",
       "2   7.166667  87.000000    4.633333   1018.666667         8.666667\n",
       "3   8.666667  71.333333    1.233333   1017.166667         6.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first batch contains the first two rows of the dataset:\n",
      " tensor([[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "        [   7.4000,   92.0000,    2.9800, 1017.8000]])\n",
      "The second batch contains the second two rows of the dataset:\n",
      " tensor([[   7.1667,   87.0000,    4.6333, 1018.6667],\n",
      "        [   8.6667,   71.3333,    1.2333, 1017.1667]])\n"
     ]
    }
   ],
   "source": [
    "# Normalise has been set to False for demo purposes\n",
    "tmp_dataset = PandasDataset(X=train_df[indp_cols], y=train_df[trgt_col], normalise=False)\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False, batch_size=2)\n",
    "display(train_df[indp_cols+[trgt_col]].head(4))\n",
    "loader_iter = tmp_loader.__iter__()\n",
    "first_batch = next(loader_iter)\n",
    "print(f\"The first batch contains the first two rows of the dataset:\\n {first_batch[1]}\")\n",
    "second_batch = next(loader_iter)\n",
    "print(f\"The second batch contains the second two rows of the dataset:\\n {second_batch[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1a__: \n",
    "* The __get_lookback function is designed to augment the _X and _y tensors with sequences of length \"lookback\".\n",
    "* Where lookback is defined as 2, feature values at time points 't' and 't-1' are required to predict values at timepoint 't+1'.\n",
    "* The code contains a bug where the dimensions of the _X and _y are incorrect - fix this\n",
    "\n",
    "__Exercise 1b__: \n",
    "* Consider why the target tensor is indexed as follows ```i+lookback-1:i+lookback```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN model is now ready to be defined. To begin with, we'll try just using the nn.RNN module, provided by Pytorch. Consider the picture of an RNN below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks):\n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/generic_rnn_term_pred.png)\n",
    "\n",
    "The blue blocks represent a single RNN computation and each computation take a set of hidden values, $a^{<t-1>}$, an input $x^{<t>}$ and produces a hidden state itself $a^{<t>}$. The final computation in the sequence produces an output, $y$.\n",
    "\n",
    "* Mapping these to the input parameters of nn.RNN, \n",
    "    * input_size: Represents the dimenion of $x^{<t>}$ i.e., this represents the feature dimension for each observation in the input sequence\n",
    "    * hidden_size: Represents the dimension of the hidden layer within the RNN\n",
    "    * num_layers: Represents the number of \"stacked\" RNNs. Note this __does not__ represent the number of RNN computations. Ignore this parameter for now it is discussed later\n",
    "    * nonlinearity: Represents the non-linear function which produces the set of hidden values $a^{<t>}$\n",
    "    * batch_first: If set to True, tells the RNN to expect tensors of dimension (batch_size, sequence_size, feature_size) else it expects (sequence_size, batch_size, feature_size)\n",
    "    * bidirectional: If set to true a 'bidirectional' RNN is defined. This is out of scope for the tutorial\n",
    "\n",
    "The computation described by a single RNN unit is defined by:\n",
    "\\begin{equation}\n",
    "    a^{<t>} = \\textrm{nonlinearity}(x^{<t>}W_{i,h} + b_{i,h} + a^{<t-1>}W_{h,h} + b_{h,h})\n",
    "\\end{equation}\n",
    "\n",
    "__Exercise__ 2: The computation described in https://pytorch.org/docs/stable/generated/torch.nn.RNN.html and https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks are identical (however, the $b_{i,h}$ bias is set to the identity in the cheatsheet). Try to reconclie the two with pen and paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 3:\n",
    "* Using the description above, set the parameters below assuming we require:\n",
    "    * Sequences of length 2 for each input\n",
    "    * The dimension of $W_{h,h}$ to be 52\n",
    "    * relu activation functions\n",
    "    * Whether the input data should be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 4:\n",
    "* The code below produces a bug. Debug it and define the VanillaRNN class\n",
    "* _Hints_:\n",
    "    * Performing the reconciliation exercise will help with this, in particular noticing that the description in Pytorch (and therefore the computation implemented in the nn.RNN function is __missing__ the computation $y = g_{2}(W_{y,a}a^{<T>} + b_{y})$) where $a^{<T>}$ is the hidden layer output from the final RNN computation\n",
    "    * Also, examine the object type produced by the RNN() call. Is it what you expect? Have a look at the Pytorch documentation to understand what is being produced\n",
    "    * Finally, examine the output of the RNN model and the output of the dataloader - what do you obserse? (The code below will help do this)\n",
    " \n",
    "```python\n",
    "first_batch = next(train_loader.__iter__())\n",
    "print(f\"First batch shape: {first_batch[1].shape}\")\n",
    "print(f\"First batch obs:\\n{first_batch[1]}\")\n",
    "print(f\"First batch trgt:\\n{first_batch[0]}\")\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1])\n",
    "    print(f\"All hidden: {mdl_pred[0].shape}\")\n",
    "    print(f\"All hidden values:\\n {mdl_pred[0]}\")\n",
    "    print(f\"Final hidden: {mdl_pred[1].shape}\")\n",
    "    print(f\"Final hidden values:\\n {mdl_pred[1]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 5:\n",
    "* Notice under \"Run summary\" a \"nan\" is returned. The training loop provided at the beginning of this script has been augmented with the functionality to push the ground truth values and predicted values from the validation set to weights and biases. Use weights and biases to debug why nans are being produced in the validation and implement the fix.\n",
    "* _Hint_:\n",
    "    * Examine the validation ground truth closely - try exporting it to a csv!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the size of the lookback\n",
    "A working RNN model for one step predict has been defined. \n",
    "\n",
    "__Exercise__ 6a:\n",
    "* Try improving performance of the model by changing the lookback size: Here, we are trying to define the 'amount' of historial time steps relevant for prediction\n",
    "\n",
    "__Exercise__ 6b:\n",
    "* Try experimenting with the output of the model. In machine learning \"auxiliary loss functions\" are often used to improve performance. Auxiliary loss functions assess the performance of a model to do a related task in order to increase the amount of gradient signal to pass to the model. For example, in healthcare, when developing a model to predict acute kidney injury (AKI), DeepMind assessed whether the model could predict the outcome of the lab test for AKI (https://www.nature.com/articles/s41586-019-1390-1). It might be reasonable to assume that predicting the next day values for humidity, windspeed and pressure would help in the prediction for mean temperature.\n",
    "* An alternate auxiliary might be to keep meantemp as the prediction target but predict the intermediatary days as well i.e., defining an RNN of the form:\n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/generic_rnn.png)\n",
    "\n",
    "* _Hint_:\n",
    "    * The first auxiliary loss will require significant modifications to pretty much all of the steps above - don't worry if you're rewriting a lot of code!\n",
    "    * The second auxiliary loss only requires alterating the indexing in the PandasTsDataset function and input dimensions to the fully connected head. Alternatively, would it be better to use a specific head for each intermeditey output?\n",
    "    * When validating, we are still only interested in the ability for the model to predict the temperature!\n",
    "    * The hyperparameters previously discussed i.e., learning rate, epochs, batch_size and the network architecture might need to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNNs\n",
    "The nn.RNN module also contains a 'num_layers' parameter. Setting 'num_layers' to greater than 1 creates a \"stacked\" RNN which is depicted below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) \n",
    "\n",
    "![alternative text](/cs/academic/phd3/xinrzhen/xinran/TA/figures/rnn_stacked.png)\n",
    "\n",
    "Stacking RNNs is similar to making MLPs deeper. One may want to stack an RNN if the raw features have different 'levels' of time dependant features as each layer extracts a non-linear relationship between the input to that layer and each layer is also recursively defined for the input sequence. For the climate example here, a stacked RNN might be required if it was hypothesised that there existed 'more complex' interactions between the four input variables than can be captured by a single non-linear layer. Furthermore, these 'more complex' interactions would have to be themselves recursive else, a deeper MLP could just be used instead to extract the time t prediction, $y_{i}$.\n",
    "\n",
    "__Exercise__ 7:\n",
    "* Experiment with different numbers of RNN layers and MLP head layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting n step values\n",
    "\n",
    "__Exercise__ 8:\n",
    "* Experiment with using the other target columns i.e. meantemp_5_step. When using meantemp_5_step as the target variable, we are building a model can can predict the temperature 5 days in advance. Using auxiliary losses might be useful here as one may expect that if the model can predict the next day more accurately, it should be able to predict the fifth day more accurately. However, again be careful not to use the 1 day predictions in your validation assessment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
